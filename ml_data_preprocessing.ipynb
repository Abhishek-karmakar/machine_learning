{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - Data Preprocessing.\n",
    "----------------------------------------\n",
    "\n",
    "Data Preprocessing - Data preprocessing is one of the most vital steps in creating the machine learning models. This will be the starndard process that needs to be done on any dataset for creating any model that we want. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the essential libraries.\n",
    "\n",
    "\t1.> Numpy - The library containts mathematial tools. We use this library to include any mathematical calculations in our code. \t\n",
    "\t2.> Matplotlib.pyplot - library to help us create nice charts in python.  \n",
    "\t3.> Pandas library - library to import and manage datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Age</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Purchased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>France</td>\n",
       "      <td>44.0</td>\n",
       "      <td>72000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spain</td>\n",
       "      <td>27.0</td>\n",
       "      <td>48000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Germany</td>\n",
       "      <td>30.0</td>\n",
       "      <td>54000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Spain</td>\n",
       "      <td>38.0</td>\n",
       "      <td>61000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Germany</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>France</td>\n",
       "      <td>35.0</td>\n",
       "      <td>58000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Spain</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>France</td>\n",
       "      <td>48.0</td>\n",
       "      <td>79000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Germany</td>\n",
       "      <td>50.0</td>\n",
       "      <td>83000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>France</td>\n",
       "      <td>37.0</td>\n",
       "      <td>67000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Country   Age   Salary Purchased\n",
       "0   France  44.0  72000.0        No\n",
       "1    Spain  27.0  48000.0       Yes\n",
       "2  Germany  30.0  54000.0        No\n",
       "3    Spain  38.0  61000.0        No\n",
       "4  Germany  40.0      NaN       Yes\n",
       "5   France  35.0  58000.0       Yes\n",
       "6    Spain   NaN  52000.0        No\n",
       "7   France  48.0  79000.0       Yes\n",
       "8  Germany  50.0  83000.0        No\n",
       "9   France  37.0  67000.0       Yes"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('/home/abhishek/projects/machine-learning/Data.csv')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this data we want to create a Matrix of Features and Dependent variable vector. Matrix of features means all the data that we need to take the decision or the independent variable. We will do it use the code X = dataset.iloc[:,:-1].values. Here : means all and :-1 means all but last. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['France', 44.0, 72000.0],\n",
       "       ['Spain', 27.0, 48000.0],\n",
       "       ['Germany', 30.0, 54000.0],\n",
       "       ['Spain', 38.0, 61000.0],\n",
       "       ['Germany', 40.0, nan],\n",
       "       ['France', 35.0, 58000.0],\n",
       "       ['Spain', nan, 52000.0],\n",
       "       ['France', 48.0, 79000.0],\n",
       "       ['Germany', 50.0, 83000.0],\n",
       "       ['France', 37.0, 67000.0]], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = dataset.iloc[:,:-1].values\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now create the dependent Variable Vector. which will be the last column which will be Y = dataset.iloc[:,3].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['No', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = dataset.iloc[:,3].values\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Missing Data\n",
    "We can see from the data above that there is missing data that we need to handle for our ML model to run correctly. In the above dataset we can see that we have one missing data in the Age column and one missing data in the salary column. We do not want to delete the row as it might containt crutial data. Another way is to handle the missing data is by taking the mean of the values in the column and putting it there. We will use the imputer class from Scikitlearn library. \n",
    "\n",
    "Imputer takes some parameters, missing values = 'NaN', Strategy = 'mean' and axis\n",
    "\n",
    "We will use imputer only on the data that we want to use it on. Upper bound is excluded. \n",
    "\n",
    "Use the transform method of the imputer class to takeout the mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['France', 44.0, 72000.0],\n",
       "       ['Spain', 27.0, 48000.0],\n",
       "       ['Germany', 30.0, 54000.0],\n",
       "       ['Spain', 38.0, 61000.0],\n",
       "       ['Germany', 40.0, 63777.77777777778],\n",
       "       ['France', 35.0, 58000.0],\n",
       "       ['Spain', 38.77777777777778, 52000.0],\n",
       "       ['France', 48.0, 79000.0],\n",
       "       ['Germany', 50.0, 83000.0],\n",
       "       ['France', 37.0, 67000.0]], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer(missing_values = 'NaN', strategy = 'mean', axis = 0)\n",
    "imputer = imputer.fit(X[:,1:3])\n",
    "X[:,1:3] = imputer.transform(X[:, 1:3])\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see that the missing data is now replaces with the mean. We can take other strategies also. We could use median or the most used value as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Data\n",
    "We have to take care of the categorical data while working on a ML model. In the above dataset we can see that there are two Columns where we have text input which has categorical data. \n",
    "\n",
    "1.> Countries has 3 categories [France, Spain, Germany] \n",
    "2.> Purchased has 2 categories [Yes, No]\n",
    "\n",
    "Since while creating mathematical models we can create formulas which will take only numerical data we need to convert this data into the Numerical Data. We need to encode the two variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 1, 2, 1, 0, 2, 0, 1, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoding Categorical Data. \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder_X = LabelEncoder()\n",
    "labelencoder_X.fit_transform(X[:,0]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to replace this array with the first column. So all we need to do it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 44.0, 72000.0],\n",
       "       [2, 27.0, 48000.0],\n",
       "       [1, 30.0, 54000.0],\n",
       "       [2, 38.0, 61000.0],\n",
       "       [1, 40.0, 63777.77777777778],\n",
       "       [0, 35.0, 58000.0],\n",
       "       [2, 38.77777777777778, 52000.0],\n",
       "       [0, 48.0, 79000.0],\n",
       "       [1, 50.0, 83000.0],\n",
       "       [0, 37.0, 67000.0]], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:,0] = labelencoder_X.fit_transform(X[:,0])\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have got our first column as all numbers but this is still not suffecient. Our formulas might take 0,1&2 as seperate values where they are just two seperate entities. So we need to prevent the machine learning algorithm from thining that one country is greater than the other. So we will introduce Dummy Variables which will have differnt columns for all the different rows. 1 means where they are present. 0 means they are not present there. \n",
    "\n",
    "We will use onehotencoder to create the dummy variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 4.40000000e+01, 7.20000000e+04],\n",
       "       [0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.00000000e+00, 2.70000000e+01, 4.80000000e+04],\n",
       "       [0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 1.00000000e+00,\n",
       "        0.00000000e+00, 3.00000000e+01, 5.40000000e+04],\n",
       "       [0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.00000000e+00, 3.80000000e+01, 6.10000000e+04],\n",
       "       [0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 1.00000000e+00,\n",
       "        0.00000000e+00, 4.00000000e+01, 6.37777778e+04],\n",
       "       [1.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 3.50000000e+01, 5.80000000e+04],\n",
       "       [0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.00000000e+00, 3.87777778e+01, 5.20000000e+04],\n",
       "       [1.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 4.80000000e+01, 7.90000000e+04],\n",
       "       [0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 1.00000000e+00,\n",
       "        0.00000000e+00, 5.00000000e+01, 8.30000000e+04],\n",
       "       [1.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 3.70000000e+01, 6.70000000e+04]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehotencoder = OneHotEncoder(categorical_features = [0])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1, 1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelencoder_Y = LabelEncoder()\n",
    "Y = labelencoder_Y.fit_transform(Y)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have now transformed all the data into numerical data. In out program we are storing all the values in the variables, dataset, X,Y. Lets look at their values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   Country   Age   Salary Purchased\n",
       " 0   France  44.0  72000.0        No\n",
       " 1    Spain  27.0  48000.0       Yes\n",
       " 2  Germany  30.0  54000.0        No\n",
       " 3    Spain  38.0  61000.0        No\n",
       " 4  Germany  40.0      NaN       Yes\n",
       " 5   France  35.0  58000.0       Yes\n",
       " 6    Spain   NaN  52000.0        No\n",
       " 7   France  48.0  79000.0       Yes\n",
       " 8  Germany  50.0  83000.0        No\n",
       " 9   France  37.0  67000.0       Yes,\n",
       " array([[1.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 4.40000000e+01, 7.20000000e+04],\n",
       "        [0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         1.00000000e+00, 2.70000000e+01, 4.80000000e+04],\n",
       "        [0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 1.00000000e+00,\n",
       "         0.00000000e+00, 3.00000000e+01, 5.40000000e+04],\n",
       "        [0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         1.00000000e+00, 3.80000000e+01, 6.10000000e+04],\n",
       "        [0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 1.00000000e+00,\n",
       "         0.00000000e+00, 4.00000000e+01, 6.37777778e+04],\n",
       "        [1.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 3.50000000e+01, 5.80000000e+04],\n",
       "        [0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         1.00000000e+00, 3.87777778e+01, 5.20000000e+04],\n",
       "        [1.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 4.80000000e+01, 7.90000000e+04],\n",
       "        [0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 1.00000000e+00,\n",
       "         0.00000000e+00, 5.00000000e+01, 8.30000000e+04],\n",
       "        [1.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 3.70000000e+01, 6.70000000e+04]]),\n",
       " array([0, 1, 0, 0, 1, 1, 0, 1, 0, 1]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset, X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data into Training set and Test Set\n",
    "\n",
    "For every Machine Learning model it is going to learn something from the correlation. Too much corelation is not good. That might not work if the data is slightly different. That we will see do later. The performance on the test set should not be very different from training set.  \n",
    "\n",
    "We'll use the library train_test_split from sklean.crossvalidation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhishek/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Splitting the data into training set and Test Set\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 1.00000000e+00,\n",
       "         0.00000000e+00, 4.00000000e+01, 6.37777778e+04],\n",
       "        [1.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 3.70000000e+01, 6.70000000e+04],\n",
       "        [0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         1.00000000e+00, 2.70000000e+01, 4.80000000e+04],\n",
       "        [0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         1.00000000e+00, 3.87777778e+01, 5.20000000e+04],\n",
       "        [1.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 4.80000000e+01, 7.90000000e+04],\n",
       "        [0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         1.00000000e+00, 3.80000000e+01, 6.10000000e+04],\n",
       "        [1.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 4.40000000e+01, 7.20000000e+04],\n",
       "        [1.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 3.50000000e+01, 5.80000000e+04]]),\n",
       " array([[0.0e+00, 1.0e+00, 0.0e+00, 1.0e+00, 0.0e+00, 3.0e+01, 5.4e+04],\n",
       "        [0.0e+00, 1.0e+00, 0.0e+00, 1.0e+00, 0.0e+00, 5.0e+01, 8.3e+04]]),\n",
       " array([1, 1, 1, 0, 1, 0, 0, 1]),\n",
       " array([0, 0]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So based on the above corelation the machine learning model will be able to predict based on the training set. It will take the decision on the test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "A lot of machine learning models are based on a concept called the euclidean distance which means is the distance between the two points in a graph. In the above dataset the sqared difference between the salaries is much higher than the squared differnce between the age. \n",
    "\n",
    "In a situation like this when we are creating the machine learning model, It is going to completely ignore the age variable because its soo small. \n",
    "\n",
    "To solve this we will convert the variables into something which is scalable. and both the columns will be turned into number from -1 to +1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'ed.jpg', width=400, height=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-1.        ,  1.        , -1.        ,  2.64575131, -0.77459667,\n",
       "          0.26306757,  0.12381479],\n",
       "        [ 1.        , -1.        ,  1.        , -0.37796447, -0.77459667,\n",
       "         -0.25350148,  0.46175632],\n",
       "        [-1.        ,  1.        , -1.        , -0.37796447,  1.29099445,\n",
       "         -1.97539832, -1.53093341],\n",
       "        [-1.        ,  1.        , -1.        , -0.37796447,  1.29099445,\n",
       "          0.05261351, -1.11141978],\n",
       "        [ 1.        , -1.        ,  1.        , -0.37796447, -0.77459667,\n",
       "          1.64058505,  1.7202972 ],\n",
       "        [-1.        ,  1.        , -1.        , -0.37796447,  1.29099445,\n",
       "         -0.0813118 , -0.16751412],\n",
       "        [ 1.        , -1.        ,  1.        , -0.37796447, -0.77459667,\n",
       "          0.95182631,  0.98614835],\n",
       "        [ 1.        , -1.        ,  1.        , -0.37796447, -0.77459667,\n",
       "         -0.59788085, -0.48214934]]),\n",
       " array([[-1.        ,  1.        , -1.        ,  2.64575131, -0.77459667,\n",
       "         -1.45882927, -0.90166297],\n",
       "        [-1.        ,  1.        , -1.        ,  2.64575131, -0.77459667,\n",
       "          1.98496442,  2.13981082]]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train,X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ Now all our data is Feature scaled. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
